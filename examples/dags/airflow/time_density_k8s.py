"""Airflow DAG generated by ecoscope-workflows"""

from datetime import datetime
from typing import Protocol

from airflow.configuration import conf
from airflow.decorators import dag, task
from airflow.models.taskinstance import TaskInstance

namespace = conf.get("kubernetes", "NAMESPACE")  # does this work?


class SupportsToParquet(Protocol):
    """Protocol for typing annotation of arguments to `return_postvalidator`
    closures defined within task scopes in this module.
    """

    def to_parquet(self): ...


@task.kubernetes(
    image="ecoscope-workflows:latest",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={
        "request_memory": "128Mi",
        "request_cpu": "500m",
        "limit_memory": "500Mi",
        "limit_cpu": 1,
    },
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def get_subjectgroup_observations_task(
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # the task itself
    from ecoscope_workflows.tasks.python.io import get_subjectgroup_observations

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["get_subjectgroup_observations"]

    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = get_subjectgroup_observations.replace(
        # this task has no arg_dependencies, therefore it does not require arg_prevalidators
        return_postvalidator=return_postvalidator,
        validate=True,
    )(
        **task_kwargs,
    )
    return serialized_result_url


@task.kubernetes(
    image="ecoscope-workflows:latest",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={
        "request_memory": "128Mi",
        "request_cpu": "500m",
        "limit_memory": "500Mi",
        "limit_cpu": 1,
    },
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def process_relocations_task(
    observations,
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # deserializers
    from ecoscope_workflows.serde import gpd_from_parquet_uri

    # the task itself
    from ecoscope_workflows.tasks.python.preprocessing import process_relocations

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["process_relocations"]

    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = process_relocations.replace(
        arg_prevalidators={
            "observations": gpd_from_parquet_uri,
        },
        return_postvalidator=return_postvalidator,
        validate=True,
    )(
        observations=observations,
        **task_kwargs,
    )
    return serialized_result_url


@task.kubernetes(
    image="ecoscope-workflows:latest",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={
        "request_memory": "128Mi",
        "request_cpu": "500m",
        "limit_memory": "500Mi",
        "limit_cpu": 1,
    },
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def relocations_to_trajectory_task(
    relocations,
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # deserializers
    from ecoscope_workflows.serde import gpd_from_parquet_uri

    # the task itself
    from ecoscope_workflows.tasks.python.preprocessing import relocations_to_trajectory

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["relocations_to_trajectory"]

    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = relocations_to_trajectory.replace(
        arg_prevalidators={
            "relocations": gpd_from_parquet_uri,
        },
        return_postvalidator=return_postvalidator,
        validate=True,
    )(
        relocations=relocations,
        **task_kwargs,
    )
    return serialized_result_url


@task.kubernetes(
    image="ecoscope-workflows:latest",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={
        "request_memory": "128Mi",
        "request_cpu": "500m",
        "limit_memory": "500Mi",
        "limit_cpu": 1,
    },
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def calculate_time_density_task(
    trajectory_gdf,
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # deserializers
    from ecoscope_workflows.serde import gpd_from_parquet_uri

    # the task itself
    from ecoscope_workflows.tasks.python.analysis import calculate_time_density

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["calculate_time_density"]

    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = calculate_time_density.replace(
        arg_prevalidators={
            "trajectory_gdf": gpd_from_parquet_uri,
        },
        return_postvalidator=return_postvalidator,
        validate=True,
    )(
        trajectory_gdf=trajectory_gdf,
        **task_kwargs,
    )
    return serialized_result_url


@dag(
    schedule=None,
    start_date=datetime(2021, 12, 1),
    catchup=False,
)
def calculate_time_density():
    # FIXME: first pass assumes tasks are already in topological order

    get_subjectgroup_observations_return = get_subjectgroup_observations_task()

    process_relocations_return = process_relocations_task(
        observations=get_subjectgroup_observations_return,
    )

    relocations_to_trajectory_return = relocations_to_trajectory_task(
        relocations=process_relocations_return,
    )

    calculate_time_density_return = calculate_time_density_task(
        trajectory_gdf=relocations_to_trajectory_return,
    )


calculate_time_density()
