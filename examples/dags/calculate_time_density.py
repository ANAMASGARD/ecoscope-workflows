"""Airflow DAG generated by `ecoscope_workflows/generate_dag.py`"""
from datetime import datetime

from airflow.configuration import conf 
from airflow.decorators import dag, task

from ecoscope_workflows.decorators import distributed as DistributedTask

namespace = conf.get("kubernetes", "NAMESPACE")  # does this work?

def import_item(): ...  # TODO: implement in workflows

# TODO: possibly need some global de/serialization fsspec filesystem
# defined here, inheriting system credentials?


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def get_earthranger_subjectgroup_observations(
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
):
    task: DistributedTask = import_item("ecoscope_workflows.tasks.python.io.get_subjectgroup_observations")
    task_kwargs = params["get_earthranger_subjectgroup_observations"]
    # something about loading registered deserializers by arg type
    # something about return_postvalidator closures
    outpath = task.replace(
        arg_prevalidators=...,  # this is a loop in itself
        return_postvalidator=...,  # set this from a storage config
        validate=True
    )(
        **task_kwargs,
    )
    return outpath


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def process_relocations(
    observations,
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
):
    task: DistributedTask = import_item("ecoscope_workflows.tasks.python.preprocessing.process_relocations")
    task_kwargs = params["process_relocations"]
    # something about loading registered deserializers by arg type
    # something about return_postvalidator closures
    outpath = task.replace(
        arg_prevalidators=...,  # this is a loop in itself
        return_postvalidator=...,  # set this from a storage config
        validate=True
    )(
        observations=observations,
        **task_kwargs,
    )
    return outpath




@dag(schedule="@daily", start_date=datetime(2021, 12, 1), catchup=False)
def calculate_time_density():
    # FIXME: first pass assumes tasks are already in topological order
    
    get_earthranger_subjectgroup_observations_return = get_earthranger_subjectgroup_observations()
    
    process_relocations_return = process_relocations(
        observations=get_earthranger_subjectgroup_observations_return,
    )
