"""Airflow DAG generated by `ecoscope_workflows/generate_dag.py`"""
from datetime import datetime

from airflow.configuration import conf 
from airflow.decorators import dag, task

from ecoscope_workflows.decorators import distributed

namespace = conf.get("kubernetes", "NAMESPACE")  # does this work?

def import_item(): ...  # TODO: implement in workflows

# TODO: possibly need some global de/serialization fsspec filesystem
# defined here, inheriting system credentials?


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def get_earthranger_subjectgroup_observations(
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
):
    # the task itself, wrapping it as `distributed`, and fetching its kwargs
    plain_task = import_item("ecoscope_workflows.tasks.python.io.get_subjectgroup_observations")
    distributed_task = distributed(plain_task)
    task_kwargs = params["get_earthranger_subjectgroup_observations"]
    
    # something about return_postvalidator closures
    serliazed_result_uri = distributed_task.replace(
        # this task has no arg_dependencies, therefore it does not require arg_prevalidators
        return_postvalidator=...,  # set this from a storage config
        validate=True
    )(
        **task_kwargs,
    )
    return serliazed_result_uri


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def process_relocations(
    observations,
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
):
    # deserializers
    from ecoscope_workflows.serde import gpd_from_parquet_uri
    
    # the task itself, wrapping it as `distributed`, and fetching its kwargs
    plain_task = import_item("ecoscope_workflows.tasks.python.preprocessing.process_relocations")
    distributed_task = distributed(plain_task)
    task_kwargs = params["process_relocations"]
    
    # something about return_postvalidator closures
    serliazed_result_uri = distributed_task.replace(
        arg_prevalidators={
            "observations": gpd_from_parquet_uri,
        },
        return_postvalidator=...,  # set this from a storage config
        validate=True
    )(
        observations=observations,
        **task_kwargs,
    )
    return serliazed_result_uri




@dag(schedule="@daily", start_date=datetime(2021, 12, 1), catchup=False)
def calculate_time_density():
    # FIXME: first pass assumes tasks are already in topological order
    
    get_earthranger_subjectgroup_observations_return = get_earthranger_subjectgroup_observations()
    
    process_relocations_return = process_relocations(
        observations=get_earthranger_subjectgroup_observations_return,
    )
    