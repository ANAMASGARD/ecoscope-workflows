"""Airflow DAG generated by `ecoscope_workflows/generate_dag.py`"""
from datetime import datetime
from typing import Protocol

from airflow.configuration import conf 
from airflow.decorators import dag, task
from airflow.models.taskinstance import TaskInstance

from ecoscope_workflows.decorators import distributed as DistributedTask

namespace = conf.get("kubernetes", "NAMESPACE")  # does this work?

def import_item(): ...  # TODO: implement in workflows

# TODO: possibly need some global de/serialization fsspec filesystem
# defined here, inheriting system credentials?

class SupportsToParquet(Protocol):
    """Protocol for typing annotation of arguments to `return_postvalidator`
    closures defined within task scopes in this module.
    """
    def to_parquet(self): ...


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def get_earthranger_subjectgroup_observations(
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # the task itself
    from ecoscope_workflows.tasks.python.io import get_subjectgroup_observations
    assert isinstance(get_subjectgroup_observations, DistributedTask)

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["get_earthranger_subjectgroup_observations"]
    
    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = get_subjectgroup_observations.replace(
        # this task has no arg_dependencies, therefore it does not require arg_prevalidators
        return_postvalidator=return_postvalidator,  
        validate=True
    )(
        **task_kwargs,
    )
    return serialized_result_url


@task.kubernetes(
    image="ecoscope:0.1.7",
    in_cluster=True,
    namespace=namespace,
    name="pod",
    container_resources={'request_memory': '128Mi', 'request_cpu': '500m', 'limit_memory': '500Mi', 'limit_cpu': 1},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def process_relocations(
    observations,
    # Context variables are passed as kwargs in TaskFlow:
    # https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#accessing-context-variables-in-decorated-tasks
    # NOTE: Airflow >= 2.8 doesn't require None as a default, but Cloud Composer is on 2.7.*
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
    ti: TaskInstance | None = None,
):
    # deserializers
    from ecoscope_workflows.serde import gpd_from_parquet_uri
    
    # the task itself
    from ecoscope_workflows.tasks.python.preprocessing import process_relocations
    assert isinstance(process_relocations, DistributedTask)

    # user-passed kwargs for the task (via airflow dag params)
    task_kwargs = params["process_relocations"]
    
    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = f"gcs://my-bucket/ecoscope/cache/dag-runs/{ti.task_id}.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = process_relocations.replace(
        arg_prevalidators={
            "observations": gpd_from_parquet_uri,
        },
        return_postvalidator=return_postvalidator,  
        validate=True
    )(
        observations=observations,
        **task_kwargs,
    )
    return serialized_result_url

@dag(
    schedule=None,
    start_date=datetime(2021, 12, 1),
    catchup=False,
)
def calculate_time_density():
    # FIXME: first pass assumes tasks are already in topological order
    
    get_earthranger_subjectgroup_observations_return = get_earthranger_subjectgroup_observations()
    
    process_relocations_return = process_relocations(
        observations=get_earthranger_subjectgroup_observations_return,
    )
    