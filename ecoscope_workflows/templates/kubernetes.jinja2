"""Airflow DAG generated by `ecoscope_workflows/generate_dag.py`"""
from datetime import datetime
from typing import Protocol

from airflow.configuration import conf 
from airflow.decorators import dag, task

from ecoscope_workflows.decorators import distributed

namespace = conf.get("kubernetes", "NAMESPACE")  # does this work?

def import_item(): ...  # TODO: implement in workflows

# TODO: possibly need some global de/serialization fsspec filesystem
# defined here, inheriting system credentials?

class SupportsToParquet(Protocol):
    """Protocol for typing annotation of arguments to `return_postvalidator`
    closures defined within task scopes in this module.
    """
    def to_parquet(self): ...

{% for t in tasks %}
@task.kubernetes(
    image="{{ t.known_task.operator.image }}",
    in_cluster=True,
    namespace=namespace,
    name="{{ t.known_task.operator.name }}",
    container_resources={{ t.known_task.operator.container_resources }},
    get_logs=True,
    log_events_on_failure=True,
    do_xcom_push=True,
)
def {{ t.known_task_name }}(
    {%- if t.arg_dependencies %}
    {% for arg in t.arg_dependencies %}{{ arg }},{% endfor -%}
    {% endif %}
    params: dict | None = None,  # Airflow DAG Params passed with `--conf` on trigger
):
    {%- if t.arg_dependencies %}
    # deserializers
    {%- for arg in t.arg_dependencies %}
    from ecoscope_workflows.serde import {{ t.arg_prevalidators[arg] }}
    {% endfor %}
    {% else %}
    {% endif -%}

    # the task itself, wrapping it as `distributed`, and fetching its kwargs
    plain_task = import_item("{{ t.known_task.importable_reference }}")
    distributed_task = distributed(plain_task)
    task_kwargs = params["{{ t.known_task_name }}"]
    
    # TODO: support various serialization types based on the return type of `plain_task`
    # for just right now we are going to assume it's always a geopandas dataframe
    def return_postvalidator(dataframe: SupportsToParquet) -> str:
        """Serializes return value of `plain_task`."""
        # NOTE: This needs to be defined as a closure here in the task scope, because
        # we want to use some task-instance specific information from the enclosing scope
        # to set the `url` defined in the body of this function below, and it would seem
        # that cannot be passed as a kwarg, because our `return_postvalidator`s need to be
        # only single-argument callables for compatibility with Pydantic's AfterValidator,
        # which we use for the internal implementation of this serialization feature.
        # FIXME: compose `url` from configured `base_path` + task-instance-specific identifier
        url = "somewhere.parquet"
        # FIXME: support authentication here, set from configurable storage model
        # (this might be more easily managed with dask-geopandas?)
        dataframe.to_parquet(url)
        return url

    serialized_result_url = distributed_task.replace(
        {%- if t.arg_dependencies %}
        arg_prevalidators={
            {% for arg in t.arg_dependencies %}"{{ arg }}": {{ t.arg_prevalidators[arg] }},{% endfor %}
        },
        {% else %}
        # this task has no arg_dependencies, therefore it does not require arg_prevalidators
        {% endif -%}
        return_postvalidator=return_postvalidator,  
        validate=True
    )(
        {%- if t.arg_dependencies %}
        {% for arg in t.arg_dependencies %}{{ arg }}={{ arg }},{% endfor -%}
        {% endif %}
        **task_kwargs,
    )
    return serialized_result_url

{% endfor -%}

@dag(
    schedule={%- if schedule %}"{{ schedule }}"{% else %}None{% endif %},
    start_date={{ start_date }},
    catchup={{ catchup }},
)
def {{ name }}():
    # FIXME: first pass assumes tasks are already in topological order
    {% for t in tasks %}
    {{ t.known_task_name }}_return = {{ t.known_task_name }}(
    {%- if t.arg_dependencies %}
        {% for arg in t.arg_dependencies %}{{ arg }}={{ t.arg_dependencies[arg] }},{% endfor %}
    )
    {% else %})
    {% endif -%}
    {% endfor -%}

